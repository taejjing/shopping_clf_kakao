{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install --user sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Reshape, Concatenate, Add\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import os, sys, re, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grpc://10.53.63.2:8470'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TPU_SERVER = '10.53.63.2'\n",
    "TPU_PORT = ':8470'\n",
    "TPU_WORKER = 'grpc://' + TPU_SERVER + TPU_PORT\n",
    "TPU_WORKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(max_features, max_len, embed_size, tg_cate_size):\n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "\n",
    "    inp_product  = Input(shape=(max_len, ))\n",
    "    embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_product)\n",
    "    embedding = Reshape(target_shape=(max_len*embed_size,))(embedding)\n",
    "    embedding = Dense(256, activation='relu')(embedding)\n",
    "    inputs.append(inp_product)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    inp_model  = Input(shape=(max_len, ))\n",
    "    embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_model)\n",
    "    embedding = Reshape(target_shape=(max_len*embed_size,))(embedding)\n",
    "    embedding = Dense(256, activation='relu')(embedding)\n",
    "    inputs.append(inp_model)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    inp_brand  = Input(shape=(max_len, ))\n",
    "    embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_brand)\n",
    "    embedding = Reshape(target_shape=(max_len*embed_size,))(embedding)\n",
    "    embedding = Dense(256, activation='relu')(embedding)\n",
    "    inputs.append(inp_brand)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    inp_maker  = Input(shape=(max_len, ))\n",
    "    embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_maker)\n",
    "    embedding = Reshape(target_shape=(max_len*embed_size,))(embedding)\n",
    "    embedding = Dense(256, activation='relu')(embedding)\n",
    "    inputs.append(inp_maker)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    inp_img = Input(shape=(2048, ))\n",
    "    feat = Dropout(0.4)(inp_img)\n",
    "    feat = Dense(256, activation='relu')(feat)\n",
    "    inputs.append(inp_img)\n",
    "    embeddings.append(feat)\n",
    "\n",
    "    x = Concatenate()(embeddings)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "\n",
    "    # decoder1 = add([x, feat])\n",
    "    # decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    output = Dense(tg_cate_size, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:      165060836      601468   101977076         868    62482292   162947068\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(list_X, y, batch_size, total_size):\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        if (start_idx + batch_size) >= total_size:\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "        \n",
    "        end_idx += batch_size\n",
    "        batch_X = []\n",
    "        for X in list_X:\n",
    "            batch_X.append(X[start_idx:end_idx])\n",
    "            \n",
    "        batch_y = np.array(y[start_idx:end_idx])\n",
    "        start_idx = end_idx        \n",
    "        \n",
    "        yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_product = h5py.File('origin/X_train_concate.h5py', 'r')\n",
    "# X_maker = h5py.File('X_train_maker.h5py', 'r')\n",
    "# X_brand = h5py.File('X_train_brand.h5py', 'r')\n",
    "# X_model = h5py.File('X_train_model.h5py', 'r')\n",
    "# X_image = h5py.File('X_train_img_feat.h5py', 'r')\n",
    "\n",
    "y_b_cate = h5py.File('origin/y_train_bcateid.h5py', 'r')\n",
    "# y_m_cate = h5py.File('y_train_mcateid.h5py', 'r')\n",
    "# y_s_cate = h5py.File('y_train_scateid.h5py', 'r')\n",
    "# y_d_cate = h5py.File('y_train_dcateid.h5py', 'r')\n",
    "\n",
    "# print(\"Product / train : {}, val : {}\".format(X_product['train'].shape, X_product['val'].shape))\n",
    "# print(\"Maker / train : {}, val : {}\".format(X_maker['train'].shape, X_maker['val'].shape))\n",
    "# print(\"Brand / train : {}, val : {}\".format(X_brand['train'].shape, X_brand['val'].shape))\n",
    "# print(\"Model / train : {}, val : {}\".format(X_model['train'].shape, X_model['val'].shape))\n",
    "# print(\"Image / train : {}, val : {}\".format(X_image['train'].shape, X_image['val'].shape))\n",
    "\n",
    "# print(\"B cate / train : {}, val : {}\".format(y_b_cate['train'].shape, y_b_cate['val'].shape))\n",
    "# print(\"M cate / train : {}, val : {}\".format(y_m_cate['train'].shape, y_m_cate['val'].shape))\n",
    "# print(\"S cate / train : {}, val : {}\".format(y_s_cate['train'].shape, y_s_cate['val'].shape))\n",
    "# print(\"D cate / train : {}, val : {}\".format(y_d_cate['train'].shape, y_d_cate['val'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product for s / train : (5055885, 32), val : (1212813, 32)\n",
      "Maker for s / train : (5055885, 32), val : (1212813, 32)\n",
      "Brand for s / train : (5055885, 32), val : (1212813, 32)\n",
      "Model for s / train : (5055885, 32), val : (1212813, 32)\n",
      "Image for s / train : (5055885, 2048), val : (1212813, 2048)\n",
      "Product for d / train : (607560, 32), val : (150332, 32)\n",
      "Maker for d / train : (607560, 32), val : (150332, 32)\n",
      "Brand for d / train : (607560, 32), val : (150332, 32)\n",
      "Model for d / train : (607560, 32), val : (150332, 32)\n",
      "Image for d / train : (607560, 2048), val : (150332, 2048)\n",
      "b cate for s / (5055885, 57) (1212813, 57)\n",
      "m cat for d / (5055885, 552) (1212813, 552)\n",
      "s cate for s / (5055885, 3191) (1212813, 3191)\n",
      "d cat for d / (607560, 405) (150332, 405)\n"
     ]
    }
   ],
   "source": [
    "X_product_s = h5py.File('./Taejin/X_product_for_s.h5py', 'r')\n",
    "X_maker_s = h5py.File('./Taejin/X_maker_for_s.h5py', 'r')\n",
    "X_brand_s = h5py.File('./Taejin/X_brand_for_s.h5py', 'r')\n",
    "X_model_s = h5py.File('./Taejin/X_model_for_s.h5py', 'r')\n",
    "X_image_s = h5py.File('./Taejin/X_image_for_s.h5py', 'r')\n",
    "\n",
    "X_product_d = h5py.File('./Taejin/X_product_for_d.h5py', 'r')\n",
    "X_maker_d = h5py.File('./Taejin/X_maker_for_d.h5py', 'r')\n",
    "X_brand_d = h5py.File('./Taejin/X_brand_for_d.h5py', 'r')\n",
    "X_model_d = h5py.File('./Taejin/X_model_for_d.h5py', 'r')\n",
    "X_image_d = h5py.File('./Taejin/X_image_for_d.h5py', 'r')\n",
    "\n",
    "y_b_cate_s = h5py.File('./Taejin/y_bcateid_rm_for_s.h5py', 'r')\n",
    "y_m_cate_s = h5py.File('./Taejin/y_mcateid_rm_for_s.h5py', 'r')\n",
    "y_s_cate_s = h5py.File('./Taejin/y_scateid_rm.h5py', 'r')\n",
    "\n",
    "y_b_cate_d = h5py.File('./Taejin/y_bcateid_rm_for_d.h5py', 'r')\n",
    "y_m_cate_d = h5py.File('./Taejin/y_mcateid_rm_for_d.h5py', 'r')\n",
    "y_s_cate_d = h5py.File('./Taejin/y_scateid_rm_for_d.h5py', 'r')\n",
    "y_d_cate_d = h5py.File('./Taejin/y_dcateid_rm.h5py', 'r')\n",
    "\n",
    "print(\"Product for s / train : {}, val : {}\".format(X_product_s['train'].shape, X_product_s['val'].shape))\n",
    "print(\"Maker for s / train : {}, val : {}\".format(X_maker_s['train'].shape, X_maker_s['val'].shape))\n",
    "print(\"Brand for s / train : {}, val : {}\".format(X_brand_s['train'].shape, X_brand_s['val'].shape))\n",
    "print(\"Model for s / train : {}, val : {}\".format(X_model_s['train'].shape, X_model_s['val'].shape))\n",
    "print(\"Image for s / train : {}, val : {}\".format(X_image_s['train'].shape, X_image_s['val'].shape))\n",
    "\n",
    "print(\"Product for d / train : {}, val : {}\".format(X_product_d['train'].shape, X_product_d['val'].shape))\n",
    "print(\"Maker for d / train : {}, val : {}\".format(X_maker_d['train'].shape, X_maker_d['val'].shape))\n",
    "print(\"Brand for d / train : {}, val : {}\".format(X_brand_d['train'].shape, X_brand_d['val'].shape))\n",
    "print(\"Model for d / train : {}, val : {}\".format(X_model_d['train'].shape, X_model_d['val'].shape))\n",
    "print(\"Image for d / train : {}, val : {}\".format(X_image_d['train'].shape, X_image_d['val'].shape))\n",
    "\n",
    "print(\"b cate for s / {} {}\".format(y_b_cate_s['train'].shape, y_b_cate_s['val'].shape))\n",
    "print(\"m cat for d / {} {}\".format(y_m_cate_s['train'].shape, y_m_cate_s['val'].shape))\n",
    "print(\"s cate for s / {} {}\".format(y_s_cate_s['train'].shape, y_s_cate_s['val'].shape))\n",
    "print(\"d cat for d / {} {}\".format(y_d_cate_d['train'].shape, y_d_cate_d['val'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 200000\n",
    "max_len = 32\n",
    "embed_size = 256\n",
    "cate_size = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_product  = Input(shape=(max_len, ))\n",
    "embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_product)\n",
    "lstm = Bidirectional(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding)\n",
    "x = GlobalMaxPool1D()(lstm)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "output = Dense(57, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inp_product, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 32, 256)           51200000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 32, 512)           1050624   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 57)                14649     \n",
      "=================================================================\n",
      "Total params: 52,396,601\n",
      "Trainable params: 52,396,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:      165060836     7608712    79500524         868    77951600   155939796\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = generate_model(max_features, max_len, embed_size, cate_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.53.63.2:8470') for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 2504109522095969079)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1828738280491675625)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 17377812499413327759)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10747940432500372)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9230945098118146750)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 16678192585012357837)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12689120182799323887)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16073087823377222854)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14347201436742216915)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15181913317033556100)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12747134493609688558)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5175286311336546626)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpu_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tpu_model.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = data_generator([X_product['train'], X_model['train'], X_brand['train'], X_maker['train'], X_image['train']], y_m_cate['train'], batch_size, len(X_product['train']))\n",
    "# valid_generator = data_generator([X_product['val'], X_model['val'], X_brand['val'], X_maker['val'],   X_image['val']], y_m_cate['val'], batch_size, len(X_product['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = data_generator([X_product_d['train'], X_model_d['train'], X_brand_d['train'], X_maker_d['train'], X_image_d['train']], y_d_cate_d['train'], batch_size, len(X_product_d['train']))\n",
    "# valid_generator = data_generator([X_product_d['val'], X_model_d['val'], X_brand_d['val'], X_maker_d['val'], X_image_d['val']], y_d_cate_d['val'], batch_size, len(X_product_d['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator([X_product_s['train'], X_model_s['train'], X_brand_s['train'], X_maker_s['train'], X_image_s['train']], y_s_cate_s['train'], batch_size, len(X_product_s['train']))\n",
    "valid_generator = data_generator([X_product_s['val'], X_model_s['val'], X_brand_s['val'], X_maker_s['val'], X_image_s['val']], y_s_cate_s['val'], batch_size, len(X_product_s['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator([X_product['train']], y_b_cate['train'], batch_size, len(X_product['train']))\n",
    "valid_generator = data_generator([X_product['val']], y_b_cate['val'], batch_size, len(X_product['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('b_model_concate.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(4096,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(4096, 32), dtype=tf.float32, name='input_20'), TensorSpec(shape=(4096, 57), dtype=tf.float32, name='dense_3_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for input_2\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 8.75177001953125 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "198/199 [============================>.] - ETA: 1s - loss: 3.5500 - acc: 0.0693INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(4096,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(4096, 32), dtype=tf.float32, name='input_20'), TensorSpec(shape=(4096, 57), dtype=tf.float32, name='dense_3_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for input_2\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 5.810992479324341 secs\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.06939, saving model to b_model_concate.h5\n",
      "INFO:tensorflow:Copying TPU weights to the CPU\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "199/199 [==============================] - 339s 2s/step - loss: 3.5504 - acc: 0.0694 - val_loss: 3.5584 - val_acc: 0.0675\n",
      "Epoch 2/20\n",
      " 35/199 [====>.........................] - ETA: 3:50 - loss: 2.9261 - acc: 0.2679"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-de4103c85b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_product\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         callbacks=[model_checkpoint])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m     ], infeed_dict)\n\u001b[0m\u001b[1;32m   1259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tpu_model.fit_generator(generator=train_generator, \n",
    "                        steps_per_epoch=int(len(X_product['train'])/batch_size)+1, \n",
    "                        validation_data=valid_generator, \n",
    "                        validation_steps=int(len(X_product['val'])/batch_size)+1, epochs=20,\n",
    "                        callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in load_b.layers:\n",
    "    print(i.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_b.layers[-6].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_b = load_model('tpu_b_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_b.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_m_input = Input(shape=(1,), name='m_input')\n",
    "new_m = Dense(256, activation='relu', name='dense_1_m1')(new_m_input)\n",
    "concate_m = Concatenate(name='concate_m1')([load_b.layers[-6].output, new_m])\n",
    "# new_m = BatchNormalization()(concate_m)\n",
    "new_m = Dense(512, activation='relu', name='dense_2_m1')(concate_m)\n",
    "new_m = Dropout(0.2, name='drop_2')(new_m)\n",
    "new_m = Dense(256, activation='relu', name='dense_4_m1')(new_m)\n",
    "new_m = Dropout(0.2, name='drop_3')(new_m)\n",
    "output = Dense(552, activation='softmax', name='dense_3_m1')(new_m)\n",
    "\n",
    "model_m = Model([load_b.layers[4].input,load_b.layers[5].input,load_b.layers[6].input,load_b.layers[7].input,load_b.layers[8].input,  new_m_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_m(list_X,input_cate, y, batch_size, total_size):\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    while True:\n",
    "        if (start_idx + batch_size) >= total_size:\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "            \n",
    "        end_idx += batch_size\n",
    "#         offset = np.random.randint(0, total_size - batch_size)\n",
    "        batch_X = []\n",
    "        for X in list_X:\n",
    "            batch_X.append(np.array(X[start_idx:end_idx]))\n",
    "            \n",
    "        cate_input = np.argmax(input_cate[start_idx:end_idx], axis=1)+1\n",
    "        batch_X.append(cate_input)    \n",
    "        batch_y = np.array(y[start_idx:end_idx])\n",
    "        start_idx = end_idx        \n",
    "        \n",
    "        yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_m = data_generator_m([X_product['train'], X_model['train'], X_brand['train'], X_maker['train'], X_image['train']],y_b_cate['train'], y_m_cate['train'], batch_size, len(X_product['train']))\n",
    "valid_generator_m = data_generator_m([X_product['val'], X_model['val'], X_brand['val'], X_maker['val'],   X_image['val']],y_b_cate['val'], y_m_cate['val'], batch_size, len(X_product['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_generator_m)[0][-1][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['acc'])\n",
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "ft_checkpoint = ModelCheckpoint('m_model_transfer.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.fit_generator(generator=train_generator_m, \n",
    "                        steps_per_epoch=int(6507854/batch_size)+1, \n",
    "                        validation_data=valid_generator_m, \n",
    "                        validation_steps=int(1626964/batch_size)+1, epochs=10,\n",
    "                        callbacks=[ft_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.save('tpu_m_transfer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S_transfer model test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "load_m = load_model('m_model_transfer.h5')\n",
    "load_m.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_2_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_3_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_4_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_5_1:0\", shape=(?, 2048), dtype=float32)\n",
      "Tensor(\"m_input:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concate_m1/concat:0\", shape=(?, 1536), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(load_m.layers[0].input)\n",
    "print(load_m.layers[1].input)\n",
    "print(load_m.layers[2].input)\n",
    "print(load_m.layers[3].input)\n",
    "print(load_m.layers[8].input)\n",
    "print(load_m.layers[-9].input)\n",
    "print(load_m.layers[-6].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_s = Input(shape=(1,), name='input_s')\n",
    "new1 = Dense(256, activation='relu')(new_s)\n",
    "concate_m = Concatenate()([load_m.layers[-6].output, new1])\n",
    "# new1 = BatchNormalization()(concate_m)\n",
    "new1 = Dense(256, activation='relu')(concate_m)\n",
    "new1 = Dropout(0.2)(new1)\n",
    "output = Dense(3191, activation='softmax')(new1)\n",
    "\n",
    "model_s = Model([load_m.layers[0].input,load_m.layers[1].input,load_m.layers[2].input,load_m.layers[3].input,load_m.layers[8].input, load_m.layers[-9].input, new_s], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "max_len = 32\n",
    "embed_size = 128\n",
    "cate_size = 3191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_s(list_X,input_cate_b, input_cate_m, y, batch_size, total_size):\n",
    "#     start_idx = 0\n",
    "#     end_idx = 0\n",
    "    while True:\n",
    "#         end_idx += batch_size\n",
    "        offset = np.random.randint(0, total_size - batch_size)\n",
    "        batch_X = []\n",
    "        for X in list_X:\n",
    "            batch_X.append(np.array(X[offset:offset+batch_size]))\n",
    "            \n",
    "        cate_input_b = np.argmax(input_cate_b[offset:offset+batch_size], axis=1)+1\n",
    "        batch_X.append(cate_input_b)\n",
    "        cate_input_m = np.argmax(input_cate_m[offset:offset+batch_size], axis=1)+1\n",
    "        batch_X.append(cate_input_m)\n",
    "        batch_y = np.array(y[offset:offset+batch_size])\n",
    "#         start_idx = end_idx        \n",
    "        \n",
    "        yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b_cate_s = h5py.File('./Taejin/y_bcateid_rm_for_s.h5py', 'r')\n",
    "y_m_cate_s = h5py.File('./Taejin/y_mcateid_rm_for_s.h5py', 'r')\n",
    "y_s_cate_s = h5py.File('./Taejin/y_scateid_rm.h5py', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_s = data_generator_s([X_product_s['train'], X_model_s['train'], X_brand_s['train'], X_maker_s['train'], X_image_s['train']],y_b_cate_s['train'], y_m_cate_s['train'],y_s_cate_s['train'], batch_size, len(X_product_s['train']))\n",
    "valid_generator_s = data_generator_s([X_product_s['val'], X_model_s['val'], X_brand_s['val'], X_maker_s['val'], X_image_s['val']],y_b_cate_s['val'], y_m_cate_s['val'], y_s_cate_s['val'], batch_size, len(X_product_s['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['accuracy'])\n",
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "s_checkpoint = ModelCheckpoint('s_model_rm.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 1.9674 - acc: 0.5543\n",
      "Epoch 00001: acc improved from -inf to 0.55557, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 801s 5s/step - loss: 1.9607 - acc: 0.5556 - val_loss: 0.7775 - val_acc: 0.7934\n",
      "Epoch 2/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 0.6921 - acc: 0.8079\n",
      "Epoch 00002: acc improved from 0.55557 to 0.80795, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 818s 5s/step - loss: 0.6916 - acc: 0.8079 - val_loss: 0.6123 - val_acc: 0.8349\n",
      "Epoch 3/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 0.5194 - acc: 0.8529\n",
      "Epoch 00003: acc improved from 0.80795 to 0.85295, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 861s 6s/step - loss: 0.5195 - acc: 0.8529 - val_loss: 0.5296 - val_acc: 0.8564\n",
      "Epoch 4/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 0.4371 - acc: 0.8751\n",
      "Epoch 00004: acc improved from 0.85295 to 0.87501, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 830s 5s/step - loss: 0.4375 - acc: 0.8750 - val_loss: 0.5107 - val_acc: 0.8618\n",
      "Epoch 5/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 0.3852 - acc: 0.8884\n",
      "Epoch 00005: acc improved from 0.87501 to 0.88851, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 836s 5s/step - loss: 0.3850 - acc: 0.8885 - val_loss: 0.5124 - val_acc: 0.8639\n",
      "Epoch 6/10\n",
      " 26/155 [====>.........................] - ETA: 9:44 - loss: 0.3740 - acc: 0.8945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-ccd687d9f33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_product_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         callbacks=[s_checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_s.fit_generator(generator=train_generator_s, \n",
    "                        steps_per_epoch=int(len(X_product_s['train'])/batch_size)+1, \n",
    "                        validation_data=valid_generator_s, \n",
    "                        validation_steps=int(len(X_product_s['val'])/batch_size)+1, epochs=10,\n",
    "                        callbacks=[s_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D model test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "load_d = load_model('s_model_rm.h5')\n",
    "load_d.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in load_d.layers:\n",
    "    print(i.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_2_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_3_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_4_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_5_2:0\", shape=(?, 2048), dtype=float32)\n",
      "Tensor(\"m_input_1:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"input_s_3:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concatenate_2_2/concat:0\", shape=(?, 1792), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(load_d.layers[4].input)\n",
    "print(load_d.layers[5].input)\n",
    "print(load_d.layers[6].input)\n",
    "print(load_d.layers[7].input)\n",
    "print(load_d.layers[8].input)\n",
    "print(load_d.layers[-10].input)\n",
    "print(load_d.layers[-7].input)\n",
    "print(load_d.layers[-4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d = Input(shape=(1,), name='input_d')\n",
    "new1 = Dense(256, activation='relu')(new_d)\n",
    "concate_d = Concatenate()([load_d.layers[-4].output, new1])\n",
    "# new1 = BatchNormalization()(concate_m)\n",
    "new1 = Dense(256, activation='relu')(concate_d)\n",
    "new1 = Dropout(0.2)(new1)\n",
    "output = Dense(405, activation='softmax')(new1)\n",
    "\n",
    "model_d = Model([load_d.layers[4].input,load_d.layers[5].input,load_d.layers[6].input,load_d.layers[7].input,load_d.layers[8].input, load_d.layers[-10].input, load_d.layers[-7].input, new_d], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_d(list_X,input_cate_b, input_cate_m, input_cate_s, y, batch_size, total_size):\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    while True:\n",
    "        if (start_idx + batch_size) >= total_size:\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "        end_idx += batch_size\n",
    "#         offset = np.random.randint(0, total_size - batch_size)\n",
    "        batch_X = []\n",
    "        for X in list_X:\n",
    "            batch_X.append(np.array(X[start_idx:end_idx]))\n",
    "            \n",
    "        cate_input_b = np.argmax(input_cate_b[start_idx:end_idx], axis=1)+1\n",
    "        batch_X.append(cate_input_b)\n",
    "        cate_input_m = np.argmax(input_cate_m[start_idx:end_idx], axis=1)+1\n",
    "        batch_X.append(cate_input_m)\n",
    "        cate_input_s = np.argmax(input_cate_s[start_idx:end_idx], axis=1)\n",
    "        batch_X.append(cate_input_s)\n",
    "        batch_y = np.array(y[start_idx:end_idx])\n",
    "        start_idx = end_idx        \n",
    "        \n",
    "        yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_d = data_generator_d([X_product_d['train'], X_model_d['train'], X_brand_d['train'], X_maker_d['train'], X_image_d['train']],y_b_cate_d['train'], y_m_cate_d['train'],y_s_cate_d['train'], y_d_cate_d['train'], batch_size, len(X_product_d['train']))\n",
    "valid_generator_d = data_generator_d([X_product_d['val'], X_model_d['val'], X_brand_d['val'], X_maker_d['val'], X_image_d['val']],y_b_cate_d['val'], y_m_cate_d['val'], y_s_cate_d['val'], y_d_cate_d['val'], batch_size, len(X_product_d['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['accuracy'])\n",
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "d_checkpoint = ModelCheckpoint('d_model_rm.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 2.6140 - acc: 0.5301\n",
      "Epoch 00001: acc improved from -inf to 0.53142, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 59s 395ms/step - loss: 2.6036 - acc: 0.5314 - val_loss: 1.5718 - val_acc: 0.6585\n",
      "Epoch 2/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 1.3651 - acc: 0.6809\n",
      "Epoch 00002: acc improved from 0.53142 to 0.68199, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 56s 374ms/step - loss: 1.3598 - acc: 0.6820 - val_loss: 1.0899 - val_acc: 0.7119\n",
      "Epoch 3/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 1.0747 - acc: 0.7331\n",
      "Epoch 00003: acc improved from 0.68199 to 0.73398, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 57s 379ms/step - loss: 1.0708 - acc: 0.7340 - val_loss: 0.9962 - val_acc: 0.7405\n",
      "Epoch 4/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.9064 - acc: 0.7631\n",
      "Epoch 00004: acc improved from 0.73398 to 0.76402, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 56s 378ms/step - loss: 0.9025 - acc: 0.7640 - val_loss: 0.9140 - val_acc: 0.7495\n",
      "Epoch 5/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.7593 - acc: 0.7926\n",
      "Epoch 00005: acc improved from 0.76402 to 0.79344, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 55s 367ms/step - loss: 0.7562 - acc: 0.7934 - val_loss: 0.7825 - val_acc: 0.7883\n",
      "Epoch 6/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.6503 - acc: 0.8174\n",
      "Epoch 00006: acc improved from 0.79344 to 0.81809, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 361ms/step - loss: 0.6476 - acc: 0.8181 - val_loss: 0.7688 - val_acc: 0.7901\n",
      "Epoch 7/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.8377\n",
      "Epoch 00007: acc improved from 0.81809 to 0.83837, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 364ms/step - loss: 0.5488 - acc: 0.8384 - val_loss: 0.6570 - val_acc: 0.8245\n",
      "Epoch 8/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8528\n",
      "Epoch 00008: acc improved from 0.83837 to 0.85343, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 365ms/step - loss: 0.4874 - acc: 0.8534 - val_loss: 0.6701 - val_acc: 0.8216\n",
      "Epoch 9/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8684\n",
      "Epoch 00009: acc improved from 0.85343 to 0.86903, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 365ms/step - loss: 0.4323 - acc: 0.8690 - val_loss: 0.6460 - val_acc: 0.8346\n",
      "Epoch 10/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8843\n",
      "Epoch 00010: acc improved from 0.86903 to 0.88483, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 363ms/step - loss: 0.3760 - acc: 0.8848 - val_loss: 0.6423 - val_acc: 0.8353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feef1e174e0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d.fit_generator(generator=train_generator_d, \n",
    "                        steps_per_epoch=int(len(X_product_d['train'])/batch_size)+1, \n",
    "                        validation_data=valid_generator_d, \n",
    "                        validation_steps=int(len(X_product_d['val'])/batch_size)+1, epochs=10,\n",
    "                        callbacks=[d_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
