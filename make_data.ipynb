{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --user h5py numpy pandas six tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from six.moves import cPickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'origin/'\n",
    "\n",
    "train_file_list = [\n",
    "    'train.chunk.01'\n",
    "    , 'train.chunk.02', 'train.chunk.03',\n",
    "    'train.chunk.04', 'train.chunk.05', 'train.chunk.06',\n",
    "    'train.chunk.07', 'train.chunk.08', 'train.chunk.09'\n",
    "]\n",
    "\n",
    "dev_file_list = [\n",
    "    'dev.chunk.01'\n",
    "]\n",
    "\n",
    "test_file_list = [\n",
    "    'test.chunk.01',\n",
    "    'test.chunk.02'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cate_maxsize(range_cate):\n",
    "    \n",
    "    b_cate_max = 57\n",
    "    m_cate_max = 552\n",
    "    s_cate_max = 3190\n",
    "    d_cate_max = 404\n",
    "    \n",
    "    if (range_cate == 'bcateid'):\n",
    "        cate_size = b_cate_max\n",
    "    elif (range_cate == 'mcateid'):\n",
    "        cate_size = m_cate_max\n",
    "        \n",
    "    # s and d have zero(originally -1)\n",
    "    elif (range_cate == 'scateid'):\n",
    "        cate_size = s_cate_max + 1\n",
    "    elif (range_cate == 'dcateid'):\n",
    "        cate_size = d_cate_max + 1\n",
    "    else :\n",
    "        print(\"wrong id...\")\n",
    "        return\n",
    "    \n",
    "    return cate_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(data_attr):\n",
    "    \n",
    "    file_list = []\n",
    "    if data_attr == 'train':\n",
    "        file_list = train_file_list\n",
    "    elif data_attr == 'dev':\n",
    "        file_list = dev_file_list\n",
    "    elif data_attr == 'test':\n",
    "        file_list = test_file_list   \n",
    "    \n",
    "    total_size = 0\n",
    "    for file in file_list:\n",
    "        with h5py.File(path+file, 'r') as read:\n",
    "            total_size += read[data_attr]['pid'].size\n",
    "    \n",
    "    return total_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data_attr, col_name, max_len=32, num_words=100000, train_val_ratio=0.8, seed_num=17):\n",
    "    \n",
    "    col_list = ['product', 'maker', 'brand', 'model']\n",
    "    cate_list= ['bcateid', 'mcateid', 'scateid', 'dcateid']\n",
    "    \n",
    "    file_list = []\n",
    "    if data_attr == 'train':\n",
    "        file_list = train_file_list\n",
    "    elif data_attr == 'dev':\n",
    "        train_val_ratio = 1\n",
    "        file_list = dev_file_list\n",
    "    elif data_attr == 'test':\n",
    "        train_val_ratio = 1\n",
    "        file_list = test_file_list\n",
    "    else:\n",
    "        print('Wrong data attr...')\n",
    "        return\n",
    "    \n",
    "    total_file_list = train_file_list + dev_file_list + test_file_list\n",
    "    \n",
    "    total_size = get_size(data_attr)\n",
    "    train_size = int(total_size * train_val_ratio)\n",
    "    val_size = total_size - train_size\n",
    "    \n",
    "    \n",
    "    print(\"Total size : {}, train size : {}, val_size : {}\".format(total_size, train_size, val_size))\n",
    "\n",
    "    # make img_feat\n",
    "    if col_name == 'img_feat':\n",
    "        !free\n",
    "        gen_file_name = 'X_'+ str(data_attr) +'_' + str(col_name) + '.h5py'\n",
    "        print(\"Make {} dataset : {}, Generated file : {}\".format(data_attr, col_name, gen_file_name))    \n",
    "        \n",
    "        with h5py.File(path+gen_file_name, 'w') as write:            \n",
    "            write.create_dataset(data_attr, (train_size, max_len), dtype='f4')\n",
    "            write.create_dataset('val', (val_size, max_len), dtype='f4')\n",
    "            print(\"train dataset shape : {}\".format(write[data_attr].shape))\n",
    "            print(\"val dataset shape : {}\".format(write['val'].shape))\n",
    "            \n",
    "            start_idx = 0\n",
    "            tr_idx = 0\n",
    "            val_idx = 0\n",
    "            end_idx = 0            \n",
    "            for file in file_list:\n",
    "                with h5py.File(path+file, 'r') as read:\n",
    "                    print(\"current file : {}\".format(file))\n",
    "                    read_data = np.array(read[data_attr][col_name])\n",
    "                    # np.random.seed(seed_num)\n",
    "                    # np.random.shuffle(read_data)\n",
    "                    print(\"image_feat's shape of cur file : {}\".format(read_data.shape))\n",
    "                    \n",
    "                    file_size = len(read_data)\n",
    "                    \n",
    "                    tmp_train_size = int(file_size * train_val_ratio)\n",
    "                    tmp_val_size = file_size - tmp_train_size\n",
    "                     \n",
    "                    end_idx += file_size\n",
    "                    \n",
    "                    print(\"start idx: {}, end idx : {}\".format(start_idx, end_idx))\n",
    "                    print(\"train : target {} / {}\".format(tr_idx, tr_idx+tmp_train_size))\n",
    "                    write[data_attr][tr_idx:tr_idx+tmp_train_size] = read_data[:tmp_train_size]\n",
    "                    print(\"val : target {} / {}\".format(val_idx, val_idx+tmp_val_size))\n",
    "                    write['val'][val_idx:val_idx+tmp_val_size] = read_data[tmp_train_size:]\n",
    "                    start_idx = end_idx\n",
    "                    tr_idx += tmp_train_size \n",
    "                    val_idx += tmp_val_size\n",
    "                    !free\n",
    "\n",
    "            print(\"Finally, saving dataset complete ! {}, shape : {}, {}\".format(gen_file_name, write[data_attr].shape, write['val'].shape))\n",
    "    \n",
    "    # make price feature\n",
    "    elif col_name == 'price':\n",
    "        !free\n",
    "        gen_file_name = 'X_'+ str(data_attr) +'_' + str(col_name) + '.h5py'\n",
    "        print(\"Make {} dataset : {}, Generated file : {}\".format(data_attr, col_name, gen_file_name))    \n",
    "        \n",
    "        with h5py.File(path+gen_file_name, 'w') as write:            \n",
    "            write.create_dataset(data_attr, (train_size, ), dtype='i4')\n",
    "            write.create_dataset('val', (val_size, ), dtype='i4')\n",
    "            print(\"train dataset shape : {}\".format(write[data_attr].shape))\n",
    "            print(\"val dataset shape : {}\".format(write['val'].shape))\n",
    "            \n",
    "            start_idx = 0\n",
    "            tr_idx = 0\n",
    "            val_idx = 0\n",
    "            end_idx = 0            \n",
    "            for file in file_list:\n",
    "                with h5py.File(path+file, 'r') as read:\n",
    "                    print(\"current file : {}\".format(file))\n",
    "                    read_data = np.array(read[data_attr][col_name])\n",
    "                    # np.random.seed(seed_num)\n",
    "                    # np.random.shuffle(read_data)\n",
    "                    print(\"image_feat's shape of cur file : {}\".format(read_data.shape))\n",
    "                    \n",
    "                    file_size = len(read_data)\n",
    "                    \n",
    "                    tmp_train_size = int(file_size * train_val_ratio)\n",
    "                    tmp_val_size = file_size - tmp_train_size\n",
    "                     \n",
    "                    end_idx += file_size\n",
    "                    \n",
    "                    print(\"start idx: {}, end idx : {}\".format(start_idx, end_idx))\n",
    "                    print(\"train : target {} / {}\".format(tr_idx, tr_idx+tmp_train_size))\n",
    "                    write[data_attr][tr_idx:tr_idx+tmp_train_size] = read_data[:tmp_train_size]\n",
    "                    print(\"val : target {} / {}\".format(val_idx, val_idx+tmp_val_size))\n",
    "                    write['val'][val_idx:val_idx+tmp_val_size] = read_data[tmp_train_size:]\n",
    "                    start_idx = end_idx\n",
    "                    tr_idx += tmp_train_size \n",
    "                    val_idx += tmp_val_size\n",
    "                    !free\n",
    "\n",
    "            print(\"Finally, saving dataset complete ! {}, shape : {}, {}\".format(gen_file_name, write[data_attr].shape, write['val'].shape))\n",
    "\n",
    "    \n",
    "    # make y data about b, m, s, d category\n",
    "    elif col_name in cate_list :\n",
    "        !free\n",
    "        gen_file_name = 'y_'+ str(data_attr) + '_' + str(col_name) + '.h5py'\n",
    "        print(\"Make {} dataset : {}, Generated file : {}\".format(data_attr, col_name, gen_file_name))    \n",
    "        with h5py.File(path+gen_file_name, 'w') as write:\n",
    "            write.create_dataset(data_attr, (train_size, get_cate_maxsize(col_name)), dtype='?')\n",
    "            write.create_dataset('val', (val_size, get_cate_maxsize(col_name)), dtype='?')\n",
    "            print(\"train dataset shape : {}\".format(write[data_attr].shape))\n",
    "            print(\"val dataset shape : {}\".format(write['val'].shape))\n",
    "            \n",
    "            start_idx = 0\n",
    "            tr_idx = 0\n",
    "            val_idx = 0\n",
    "            end_idx = 0  \n",
    "            for file in file_list:\n",
    "                with h5py.File(path+file, 'r') as read:\n",
    "                    print(\"current file : {}\".format(file))\n",
    "                    read_data = np.array(read[data_attr][col_name])\n",
    "                    # np.random.seed(seed_num)\n",
    "                    # np.random.shuffle(read_data)\n",
    "                    print(\"cate's shape of cur file (before categorical): {}\".format(read_data.shape))\n",
    "                    \n",
    "                    file_size = len(read_data)\n",
    "                    tmp_train_size = int(file_size * train_val_ratio)\n",
    "                    tmp_val_size = file_size - tmp_train_size\n",
    "                    end_idx += file_size\n",
    "                    \n",
    "                    print(\"start idx: {}, end idx : {}\".format(start_idx, end_idx))                    \n",
    "                    \n",
    "                    if col_name in ('bcateid', 'mcateid'):\n",
    "                        to_cate = to_categorical(read_data, num_classes = get_cate_maxsize(col_name) + 1) # include zero\n",
    "                        print(\"shape (after categorical): {}\".format(to_cate.shape))\n",
    "                        write[data_attr][tr_idx:tr_idx+tmp_train_size] = to_cate[:tmp_train_size, 1:].astype('?') # delete zero columns\n",
    "                        write['val'][val_idx:val_idx+tmp_val_size] = to_cate[tmp_train_size:, 1:].astype('?')\n",
    "                        \n",
    "                    elif col_name in ('scateid', 'dcateid'):\n",
    "                        read_data[read_data == -1] = 0 # replace -1 to 0\n",
    "                        to_cate = to_categorical(read_data, num_classes = get_cate_maxsize(col_name))\n",
    "                        write[data_attr][tr_idx:tr_idx+tmp_train_size] = to_cate[:tmp_train_size].astype('?')\n",
    "                        write['val'][val_idx:val_idx+tmp_val_size] = to_cate[tmp_train_size:].astype('?')\n",
    "                        \n",
    "                    print(\"train : target {} / {}\".format(tr_idx, tr_idx+tmp_train_size))\n",
    "                    print(\"val : target {} / {}\".format(val_idx, val_idx+tmp_val_size))\n",
    "                    start_idx = end_idx\n",
    "                    tr_idx += tmp_train_size \n",
    "                    val_idx += tmp_val_size\n",
    "                    !free\n",
    "                    \n",
    "            print(\"Finally, saving dataset complete ! {}, shape : {}, {}\".format(gen_file_name, write[data_attr].shape, write['val'].shape))\n",
    "            \n",
    "\n",
    "    \n",
    "    # make input data ( product, maker, brand, model )\n",
    "    elif col_name in col_list:     \n",
    "        !free\n",
    "        gen_file_name = 'X_' + str(data_attr) + '_' + str(col_name) + '.h5py'\n",
    "        print(\"Make {} dataset : {}, Generated file : {}\".format(data_attr, col_name, gen_file_name))    \n",
    "   \n",
    "        with h5py.File(path+gen_file_name, 'w') as write:\n",
    "            write.create_dataset(data_attr, (train_size, max_len), dtype='i4')\n",
    "            write.create_dataset('val', (val_size, max_len), dtype='i4')\n",
    "            print(\"train dataset shape : {}\".format(write[data_attr].shape))\n",
    "            print(\"val dataset shape : {}\".format(write['val'].shape))        \n",
    "\n",
    "            tok_data = Tokenizer(num_words=num_words)\n",
    "            total_list = []\n",
    "\n",
    "            print(\"Start fit on text... about {}\".format(col_name))\n",
    "            \n",
    "            for file in total_file_list:\n",
    "                with h5py.File(path+file, 'r') as read:\n",
    "                    read_data = np.array(read[file.split('.')[0]][col_name]) # read file \n",
    "                    # np.random.seed(seed_num)\n",
    "                    # np.random.shuffle(read_data)                    \n",
    "                    print(\"current file : {}\".format(file))\n",
    "                    data_list = [ s.decode('utf-8') for s in read_data ]\n",
    "                    if (file.split('.')[0] == data_attr):\n",
    "                        print(\"{} file is stored in total_list...\".format(file))\n",
    "                        total_list += data_list\n",
    "                    \n",
    "                    tok_data.fit_on_texts(data_list)\n",
    "\n",
    "                    !free\n",
    "\n",
    "            print(\"fit on text Done...\")\n",
    "            print(\"Start to make sequence...\")\n",
    "            tokenized = tok_data.texts_to_sequences(total_list)\n",
    "            !free\n",
    "            print(\"padding...\")\n",
    "            X_tr_data = pad_sequences(tokenized, maxlen=max_len)\n",
    "            print(\"Shape of saving dataset : {}\".format(X_tr_data.shape))\n",
    "            !free\n",
    "\n",
    "            print(\"Save dataset...\")\n",
    "            \n",
    "            # train data / val data\n",
    "            chunk_size = 1000000\n",
    "            file_cnt = int(total_size / chunk_size)\n",
    "            if total_size % chunk_size != 0:\n",
    "                file_cnt += 1\n",
    "            st = 0\n",
    "            ed = 0\n",
    "            \n",
    "            for i in range(file_cnt):\n",
    "                if i == file_cnt - 1 :\n",
    "                    \n",
    "                    tr_st  = i * int(chunk_size * train_val_ratio)\n",
    "                    val_st = i * int(chunk_size * round(1 - train_val_ratio, 1))\n",
    "                    div = int((total_size % chunk_size) * train_val_ratio)\n",
    "                    st = i * chunk_size\n",
    "                    ed = (i+1) * chunk_size                    \n",
    "         \n",
    "                    print(\"train : target {} / {}, origin {} / {}\".format(tr_st, train_size, st, st+div))\n",
    "                    print(\"val : target {} / {}, origin {} / {}\".format(val_st, val_size, st+div, total_size))\n",
    "                    \n",
    "                    write[data_attr][tr_st:train_size] = X_tr_data[st:st+div]\n",
    "                    write['val'][val_st:val_size] = X_tr_data[st+div:total_size]\n",
    "                    \n",
    "                    break\n",
    "                else:\n",
    "                    ed += chunk_size\n",
    "                    div = int(chunk_size * train_val_ratio)\n",
    "                    \n",
    "                    tr_st = int(st * train_val_ratio)\n",
    "                    tr_ed = int(ed * train_val_ratio)\n",
    "                    val_st = int(st * round(1-train_val_ratio, 1))\n",
    "                    val_ed = int(ed * round(1-train_val_ratio, 1))\n",
    "                    \n",
    "                    print(\"train : target {} / {}, origin {} / {}\".format(tr_st, tr_ed, st, st+div))\n",
    "                    print(\"val : target {} / {}, origin {} / {}\".format(val_st, val_ed, st+div, ed))\n",
    "                    \n",
    "                    write[data_attr][tr_st:tr_ed] = X_tr_data[st:st+div]\n",
    "                    write['val'][val_st:val_ed] = X_tr_data[st+div:ed]\n",
    "                    st = ed\n",
    "                    \n",
    "            print(\"Finally, saving dataset complete ! : {}\".format(gen_file_name))\n",
    "\n",
    "        del tok_data\n",
    "        del total_list\n",
    "        del tokenized\n",
    "        del X_tr_data\n",
    "    \n",
    "    # make concate string\n",
    "    elif col_name == 'concate':\n",
    "        !free\n",
    "        gen_file_name = 'X_' + str(data_attr) + '_' + str(col_name) + '.h5py'\n",
    "        print(\"Make {} dataset : {}, Generated file : {}\".format(data_attr, col_name, gen_file_name))    \n",
    "   \n",
    "        with h5py.File(path+gen_file_name, 'w') as write:\n",
    "            write.create_dataset(data_attr, (train_size, max_len), dtype='i4')\n",
    "            write.create_dataset('val', (val_size, max_len), dtype='i4')\n",
    "            print(\"train dataset shape : {}\".format(write[data_attr].shape))\n",
    "            print(\"val dataset shape : {}\".format(write['val'].shape))        \n",
    "\n",
    "            tok_data = Tokenizer(num_words=num_words)\n",
    "            total_list = []\n",
    "\n",
    "            print(\"Start fit on text... about {}\".format(col_name))\n",
    "            \n",
    "            for file in total_file_list:\n",
    "                with h5py.File(path+file, 'r') as read:\n",
    "                    print(\"current file : {}\".format(file))\n",
    "                    print(\"target col : {}\".format('product'))\n",
    "                    target = pd.Series(read[file.split('.')[0]]['product'][:], dtype='O').str.decode('utf-8') #product\n",
    "                    print(\"current col : {}\".format('brand'))\n",
    "                    other = pd.Series(read[file.split('.')[0]]['brand'][:], dtype='O').str.decode('utf-8') # read file\n",
    "                    target = target.str.cat(other, sep=' ')\n",
    "                    print(\"concate data shape : {}\".format(target.shape))    \n",
    "                    # read_data = np.array(read[file.split('.')[0]][col_name]) # read file \n",
    "                    # np.random.seed(seed_num)\n",
    "                    # np.random.shuffle(read_data)                    \n",
    "                    data_list = [ s for s in target ]\n",
    "                    \n",
    "                    if (file.split('.')[0] == data_attr):\n",
    "                        print(\"{} file is stored in total_list...\".format(file))\n",
    "                        total_list += data_list\n",
    "                    \n",
    "                    tok_data.fit_on_texts(data_list)\n",
    "\n",
    "                    !free\n",
    "\n",
    "            print(\"fit on text Done...\")\n",
    "            print(\"Start to make sequence...\")\n",
    "            tokenized = tok_data.texts_to_sequences(total_list)\n",
    "            !free\n",
    "            print(\"padding...\")\n",
    "            X_tr_data = pad_sequences(tokenized, maxlen=max_len)\n",
    "            print(\"Shape of saving dataset : {}\".format(X_tr_data.shape))\n",
    "            !free\n",
    "\n",
    "            print(\"Save dataset...\")\n",
    "            \n",
    "            # train data / val data\n",
    "            chunk_size = 1000000\n",
    "            file_cnt = int(total_size / chunk_size)\n",
    "            if total_size % chunk_size != 0:\n",
    "                file_cnt += 1\n",
    "            st = 0\n",
    "            ed = 0\n",
    "            \n",
    "            for i in range(file_cnt):\n",
    "                if i == file_cnt - 1 :\n",
    "                    \n",
    "                    tr_st  = i * int(chunk_size * train_val_ratio)\n",
    "                    val_st = i * int(chunk_size * round(1 - train_val_ratio, 1))\n",
    "                    div = int((total_size % chunk_size) * train_val_ratio)\n",
    "                    st = i * chunk_size\n",
    "                    ed = (i+1) * chunk_size                    \n",
    "         \n",
    "                    print(\"train : target {} / {}, origin {} / {}\".format(tr_st, train_size, st, st+div))\n",
    "                    print(\"val : target {} / {}, origin {} / {}\".format(val_st, val_size, st+div, total_size))\n",
    "                    \n",
    "                    write[data_attr][tr_st:train_size] = X_tr_data[st:st+div]\n",
    "                    write['val'][val_st:val_size] = X_tr_data[st+div:total_size]\n",
    "                    \n",
    "                    break\n",
    "                else:\n",
    "                    ed += chunk_size\n",
    "                    div = int(chunk_size * train_val_ratio)\n",
    "                    \n",
    "                    tr_st = int(st * train_val_ratio)\n",
    "                    tr_ed = int(ed * train_val_ratio)\n",
    "                    val_st = int(st * round(1-train_val_ratio, 1))\n",
    "                    val_ed = int(ed * round(1-train_val_ratio, 1))\n",
    "                    \n",
    "                    print(\"train : target {} / {}, origin {} / {}\".format(tr_st, tr_ed, st, st+div))\n",
    "                    print(\"val : target {} / {}, origin {} / {}\".format(val_st, val_ed, st+div, ed))\n",
    "                    \n",
    "                    write[data_attr][tr_st:tr_ed] = X_tr_data[st:st+div]\n",
    "                    write['val'][val_st:val_ed] = X_tr_data[st+div:ed]\n",
    "                    st = ed\n",
    "                    \n",
    "            print(\"Finally, saving dataset complete ! : {}\".format(gen_file_name))\n",
    "\n",
    "        del tok_data\n",
    "        del total_list\n",
    "        del tokenized\n",
    "        del X_tr_data\n",
    "        \n",
    "    elif not col_name in col_list+cate_list :\n",
    "        print(\"Wrong col name...\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:      206357056     7687752    96778140       10876   101891164   196835908\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size : 8134818, train size : 6507854, val_size : 1626964\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     2129712    83370120       10872   120857224   202378484\n",
      "Swap:             0           0           0\n",
      "Make train dataset : concate, Generated file : X_train_concate.h5py\n",
      "train dataset shape : (6507854, 32)\n",
      "val dataset shape : (1626964, 32)\n",
      "Start fit on text... about concate\n",
      "current file : train.chunk.01\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "train.chunk.01 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     2801968    82697896       10872   120857192   201706220\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.02\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "train.chunk.02 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     3446604    82053272       10872   120857180   201061596\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.03\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "train.chunk.03 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     3853444    81646192       10872   120857420   200654760\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.04\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "train.chunk.04 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    11688324    73811248       10872   120857484   192819876\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.05\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "train.chunk.05 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    12409964    73089624       10872   120857468   192098228\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.06\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "train.chunk.06 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    12695108    72804440       10872   120857508   191813104\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.07\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "train.chunk.07 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    12943552    72556020       10872   120857484   191564664\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.08\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "train.chunk.08 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    13166928    72332592       10872   120857536   191341272\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.09\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (134818,)\n",
      "train.chunk.09 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    13137876    72361648       10872   120857532   191370320\n",
      "Swap:             0           0           0\n",
      "current file : dev.chunk.01\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (507783,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    13272368    72227164       10872   120857524   191235832\n",
      "Swap:             0           0           0\n",
      "current file : test.chunk.01\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    13469936    72029544       10872   120857576   191038236\n",
      "Swap:             0           0           0\n",
      "current file : test.chunk.02\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (526523,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    13332128    72167364       10872   120857564   191176052\n",
      "Swap:             0           0           0\n",
      "fit on text Done...\n",
      "Start to make sequence...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    14614396    70885036       10872   120857624   189893820\n",
      "Swap:             0           0           0\n",
      "padding...\n",
      "Shape of saving dataset : (8134818, 32)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056    15664044    69835400       10872   120857612   188844172\n",
      "Swap:             0           0           0\n",
      "Save dataset...\n",
      "train : target 0 / 800000, origin 0 / 800000\n",
      "val : target 0 / 200000, origin 800000 / 1000000\n",
      "train : target 800000 / 1600000, origin 1000000 / 1800000\n",
      "val : target 200000 / 400000, origin 1800000 / 2000000\n",
      "train : target 1600000 / 2400000, origin 2000000 / 2800000\n",
      "val : target 400000 / 600000, origin 2800000 / 3000000\n",
      "train : target 2400000 / 3200000, origin 3000000 / 3800000\n",
      "val : target 600000 / 800000, origin 3800000 / 4000000\n",
      "train : target 3200000 / 4000000, origin 4000000 / 4800000\n",
      "val : target 800000 / 1000000, origin 4800000 / 5000000\n",
      "train : target 4000000 / 4800000, origin 5000000 / 5800000\n",
      "val : target 1000000 / 1200000, origin 5800000 / 6000000\n",
      "train : target 4800000 / 5600000, origin 6000000 / 6800000\n",
      "val : target 1200000 / 1400000, origin 6800000 / 7000000\n",
      "train : target 5600000 / 6400000, origin 7000000 / 7800000\n",
      "val : target 1400000 / 1600000, origin 7800000 / 8000000\n",
      "train : target 6400000 / 6507854, origin 8000000 / 8107854\n",
      "val : target 1600000 / 1626964, origin 8107854 / 8134818\n",
      "Finally, saving dataset complete ! : X_train_concate.h5py\n"
     ]
    }
   ],
   "source": [
    "make_dataset('train', col_name='product', max_len=32, num_words=100000)\n",
    "make_dataset('train', col_name='maker', max_len=32, num_words=100000)\n",
    "make_dataset('train', col_name='brand', max_len=32, num_words=100000)\n",
    "make_dataset('train', col_name='model', max_len=32, num_words=100000)\n",
    "make_dataset('train', col_name='concate', max_len=32, num_words=100000)\n",
    "\n",
    "# make_dataset('train', col_name='price')\n",
    "# make_dataset('train', col_name='img_feat', max_len=2048)\n",
    "\n",
    "# make_dataset('train', col_name='bcateid')\n",
    "# make_dataset('train', col_name='mcateid')\n",
    "# make_dataset('train', col_name='scateid')\n",
    "# make_dataset('train', col_name='dcateid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size : 507783, train size : 507783, val_size : 0\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     7046740    89396296       10876   109914020   197461476\n",
      "Swap:             0           0           0\n",
      "Make dev dataset : concate, Generated file : X_dev_concate.h5py\n",
      "train dataset shape : (507783, 32)\n",
      "val dataset shape : (0, 32)\n",
      "Start fit on text... about concate\n",
      "current file : train.chunk.01\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     7638740    88804516       10876   109913800   196869476\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.02\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8019600    88423712       10876   109913744   196488612\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.03\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8055816    88387524       10876   109913716   196452388\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.04\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8117488    88325788       10876   109913780   196390696\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.05\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8717328    87725944       10876   109913784   195790864\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.06\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8774548    87668692       10876   109913816   195733616\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.07\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8858836    87584396       10876   109913824   195649348\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.08\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8911400    87531808       10876   109913848   195596768\n",
      "Swap:             0           0           0\n",
      "current file : train.chunk.09\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (134818,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8695532    87747664       10876   109913860   195812644\n",
      "Swap:             0           0           0\n",
      "current file : dev.chunk.01\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (507783,)\n",
      "dev.chunk.01 file is stored in total_list...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     8811940    87631272       10876   109913844   195696264\n",
      "Swap:             0           0           0\n",
      "current file : test.chunk.01\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (1000000,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     9088016    87350912       10876   109918128   195420152\n",
      "Swap:             0           0           0\n",
      "current file : test.chunk.02\n",
      "target col : product\n",
      "current col : brand\n",
      "concate data shape : (526523,)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     9223240    87215712       10876   109918104   195284972\n",
      "Swap:             0           0           0\n",
      "fit on text Done...\n",
      "Start to make sequence...\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     9237696    87201216       10876   109918144   195270512\n",
      "Swap:             0           0           0\n",
      "padding...\n",
      "Shape of saving dataset : (507783, 32)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:      206357056     9311964    87126952       10876   109918140   195196252\n",
      "Swap:             0           0           0\n",
      "Save dataset...\n",
      "train : target 0 / 507783, origin 0 / 507783\n",
      "val : target 0 / 0, origin 507783 / 507783\n",
      "Finally, saving dataset complete ! : X_dev_concate.h5py\n"
     ]
    }
   ],
   "source": [
    "# make_dataset('dev', col_name='product', max_len=32, num_words=100000)\n",
    "# make_dataset('dev', col_name='maker', max_len=32, num_words=100000)\n",
    "# make_dataset('dev', col_name='brand', max_len=32, num_words=100000)\n",
    "# make_dataset('dev', col_name='model', max_len=32, num_words=100000)\n",
    "\n",
    "make_dataset('dev', col_name='concate', max_len=32, num_words=200000)\n",
    "\n",
    "# make_dataset('dev', col_name='img_feat', max_len=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_dataset('test', col_name='product', max_len=32, num_words=100000)\n",
    "# make_dataset('test', col_name='maker', max_len=32, num_words=100000)\n",
    "# make_dataset('test', col_name='brand', max_len=32, num_words=100000)\n",
    "# make_dataset('test', col_name='model', max_len=32, num_words=100000)\n",
    "\n",
    "# make_dataset('test', col_name='price')\n",
    "# make_dataset('test', col_name='img_feat', max_len=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!freedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h5py.File('origin/train.chunk.01', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h['train']['product'][1].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h5py.File('y_train_scateid.h5py', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array(h['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.argmax(aa, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ss[ss == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(h['train']['product'][:], dtype='O').str.decode('utf-8')\n",
    "# s2 = pd.Series(h['train']['maker'][:10], dtype='O').str.decode('utf-8')\n",
    "# s3 = pd.Series(h['train']['brand'][:10], dtype='O').str.decode('utf-8')\n",
    "# s4 = pd.Series(h['train']['model'][:10], dtype='O').str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0                      직소퍼즐 - 1000조각 바다거북의 여행 (PL1275)\n",
       " 1    [모리케이스]아이폰6S/6S+ tree farm101 - 다이어리케이스[바보사랑][...\n",
       " 2                              크리비아 기모 3부 속바지 GLG4314P\n",
       " 3        [하프클럽/잭앤질]남성 솔리드 절개라인 포인트 포켓 팬츠 31133PT002_NA\n",
       " 4                          코드프리혈당시험지50매/코드프리시험지/최장유효기간\n",
       " 5                    아트박스 POOM/낭만창고  idk385-시원한 맥주 캬하~\n",
       " 6                    데버스 뉴 캠핑 BBQ 글러브 DVC E1209N 캠핑 등산\n",
       " 7                          엘르스포츠 여성 비키니2PCS ETFLB06NVY\n",
       " 8             [패션플러스][GEOX][GEOX] 제옥스 GH-405 블랙펄  클러치백\n",
       " 9                          [아트박스 POOM/꾸밈] iz099-우럭아왜우럭\n",
       " dtype: object]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s1.str.cat(s2, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    상품상세설명 참조\n",
       "1    MORY|해당없음\n",
       "2             \n",
       "3       ㈜크리스패션\n",
       "4           기타\n",
       "5             \n",
       "6           기타\n",
       "7           기타\n",
       "8          제옥스\n",
       "9           꾸밈\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    퍼즐라이프\n",
       "1     바보사랑\n",
       "2     크리비아\n",
       "3      잭앤질\n",
       "4         \n",
       "5         \n",
       "6         \n",
       "7    엘르스포츠\n",
       "8      제옥스\n",
       "9       꾸밈\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           퍼즐라이프 직소퍼즐 바다거북의 여행\n",
       "1    아이폰6S/6S+ tree farm101 - 다이어리케이스|아이폰6S/6S+\n",
       "2                       크리비아 기모 3부 속바지 GLG4314P\n",
       "3     [잭앤질] 남성 솔리드 절개라인 포인트 포켓 팬츠 31133PT002_NA\n",
       "4                              SD코드프리혈당시험지[50매]\n",
       "5             아트박스 POOM/낭만창고  idk385-시원한 맥주 캬하~\n",
       "6                                            기타\n",
       "7                                    ETFLB06NVY\n",
       "8                                              \n",
       "9                           인테리어액자-iz099-우럭아왜우럭\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
