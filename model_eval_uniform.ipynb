{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/18/d9/bea927c86bf78d583d517f24cbc87606cb333bfb3a5c99cb85b547305f0f/scikit_learn-0.20.2-cp35-cp35m-manylinux1_x86_64.whl (5.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.3MB 253kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.13.3 (from scikit-learn->sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/ab/19/c0ad5b9183ef97030edd6297d1726525ff2c369a09fbb6ea52a1e616ffd6/scipy-1.2.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.8.2 (from scikit-learn->sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/86/04/bd774106ae0ae1ada68c67efe89f1a16b2aa373cc2db15d974002a9f136d/numpy-1.15.4-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/kitto727/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: numpy, scipy, scikit-learn, sklearn\n",
      "Successfully installed numpy-1.15.4 scikit-learn-0.20.2 scipy-1.2.0 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --user sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Reshape, Concatenate, Add\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import os, sys, re, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grpc://10.35.75.2:8470'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TPU_SERVER = '10.35.75.2'\n",
    "TPU_PORT = ':8470'\n",
    "TPU_WORKER = 'grpc://' + TPU_SERVER + TPU_PORT\n",
    "TPU_WORKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(max_features, max_len, embed_size, tg_cate_size):\n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "\n",
    "    inp_product  = Input(shape=(max_len, ))\n",
    "    embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_product)\n",
    "    embedding = Reshape(target_shape=(max_len*embed_size,))(embedding)\n",
    "    embedding = Dense(256, activation='relu')(embedding)\n",
    "    inputs.append(inp_product)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    inp_model  = Input(shape=(max_len, ))\n",
    "    embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_model)\n",
    "    embedding = Reshape(target_shape=(max_len*embed_size,))(embedding)\n",
    "    embedding = Dense(256, activation='relu')(embedding)\n",
    "    inputs.append(inp_model)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    inp_brand  = Input(shape=(max_len, ))\n",
    "    embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_brand)\n",
    "    embedding = Reshape(target_shape=(max_len*embed_size,))(embedding)\n",
    "    embedding = Dense(256, activation='relu')(embedding)\n",
    "    inputs.append(inp_brand)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    inp_maker  = Input(shape=(max_len, ))\n",
    "    embedding = Embedding(max_features, embed_size, input_length=max_len)(inp_maker)\n",
    "    embedding = Reshape(target_shape=(max_len*embed_size,))(embedding)\n",
    "    embedding = Dense(256, activation='relu')(embedding)\n",
    "    inputs.append(inp_maker)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    inp_img = Input(shape=(2048, ))\n",
    "    feat = Dropout(0.4)(inp_img)\n",
    "    feat = Dense(256, activation='relu')(feat)\n",
    "    inputs.append(inp_img)\n",
    "    embeddings.append(feat)\n",
    "\n",
    "    x = Concatenate()(embeddings)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "\n",
    "    # decoder1 = add([x, feat])\n",
    "    # decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    output = Dense(tg_cate_size, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:      165060836      601468   101977076         868    62482292   162947068\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(list_X, y, batch_size, total_size):\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        if (start_idx + batch_size) >= total_size:\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "        \n",
    "        end_idx += batch_size\n",
    "        batch_X = []\n",
    "        for X in list_X:\n",
    "            batch_X.append(X[start_idx:end_idx])\n",
    "            \n",
    "        batch_y = np.array(y[start_idx:end_idx])\n",
    "        start_idx = end_idx        \n",
    "        \n",
    "        yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product / train : (6507854, 32), val : (1626964, 32)\n",
      "Maker / train : (6507854, 32), val : (1626964, 32)\n",
      "Brand / train : (6507854, 32), val : (1626964, 32)\n",
      "Model / train : (6507854, 32), val : (1626964, 32)\n",
      "Image / train : (6507854, 2048), val : (1626964, 2048)\n",
      "job_Product / train : (869557, 32)\n",
      "job_Maker / train : (869557, 32)\n",
      "job_Brand / train : (869557, 32)\n",
      "job_Model / train : (869557, 32)\n",
      "job_Image / train : (869557, 2048)\n",
      "B cate / train : (6507854, 57), val : (1626964, 57)\n",
      "M cate / train : (6507854, 552), val : (1626964, 552)\n",
      "S cate / train : (6507854, 3191), val : (1626964, 3191)\n",
      "D cate / train : (6507854, 405), val : (1626964, 405)\n",
      "D cate / train : (869557, 57)\n"
     ]
    }
   ],
   "source": [
    "X_product = h5py.File('origin/X_train_product.h5py', 'r')\n",
    "X_maker = h5py.File('origin/X_train_maker.h5py', 'r')\n",
    "X_brand = h5py.File('origin/X_train_brand.h5py', 'r')\n",
    "X_model = h5py.File('origin/X_train_model.h5py', 'r')\n",
    "X_image = h5py.File('origin/X_train_img_feat.h5py', 'r')\n",
    "\n",
    "X_job_product = h5py.File('bcate/X_train_product_for_b.h5py', 'r')\n",
    "X_job_maker = h5py.File('bcate/X_train_maker_for_b.h5py', 'r')\n",
    "X_job_brand = h5py.File('bcate/X_train_brand_for_b.h5py', 'r')\n",
    "X_job_model = h5py.File('bcate/X_train_model_for_b.h5py', 'r')\n",
    "X_job_image = h5py.File('bcate/X_img_for_b.h5py', 'r')\n",
    "\n",
    "y_b_cate = h5py.File('origin/y_train_bcateid.h5py', 'r')\n",
    "y_m_cate = h5py.File('origin/y_train_mcateid.h5py', 'r')\n",
    "y_s_cate = h5py.File('origin/y_train_scateid.h5py', 'r')\n",
    "y_d_cate = h5py.File('origin/y_train_dcateid.h5py', 'r')\n",
    "\n",
    "y_b_job = h5py.File('bcate/y_bcateid_for_b.h5py', 'r')\n",
    "\n",
    "print(\"Product / train : {}, val : {}\".format(X_product['train'].shape, X_product['val'].shape))\n",
    "print(\"Maker / train : {}, val : {}\".format(X_maker['train'].shape, X_maker['val'].shape))\n",
    "print(\"Brand / train : {}, val : {}\".format(X_brand['train'].shape, X_brand['val'].shape))\n",
    "print(\"Model / train : {}, val : {}\".format(X_model['train'].shape, X_model['val'].shape))\n",
    "print(\"Image / train : {}, val : {}\".format(X_image['train'].shape, X_image['val'].shape))\n",
    "\n",
    "print(\"job_Product / train : {}\".format(X_job_product['train'].shape))\n",
    "print(\"job_Maker / train : {}\".format(X_job_maker['train'].shape))\n",
    "print(\"job_Brand / train : {}\".format(X_job_brand['train'].shape))\n",
    "print(\"job_Model / train : {}\".format(X_job_model['train'].shape))\n",
    "print(\"job_Image / train : {}\".format(X_job_image['train'].shape))\n",
    "\n",
    "print(\"B cate / train : {}, val : {}\".format(y_b_cate['train'].shape, y_b_cate['val'].shape))\n",
    "print(\"M cate / train : {}, val : {}\".format(y_m_cate['train'].shape, y_m_cate['val'].shape))\n",
    "print(\"S cate / train : {}, val : {}\".format(y_s_cate['train'].shape, y_s_cate['val'].shape))\n",
    "print(\"D cate / train : {}, val : {}\".format(y_d_cate['train'].shape, y_d_cate['val'].shape))\n",
    "\n",
    "print(\"D cate / train : {}\".format(y_b_job['train'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product for s / train : (5055885, 32), val : (1212813, 32)\n",
      "Maker for s / train : (5055885, 32), val : (1212813, 32)\n",
      "Brand for s / train : (5055885, 32), val : (1212813, 32)\n",
      "Model for s / train : (5055885, 32), val : (1212813, 32)\n",
      "Image for s / train : (5055885, 2048), val : (1212813, 2048)\n",
      "Product for d / train : (607560, 32), val : (150332, 32)\n",
      "Maker for d / train : (607560, 32), val : (150332, 32)\n",
      "Brand for d / train : (607560, 32), val : (150332, 32)\n",
      "Model for d / train : (607560, 32), val : (150332, 32)\n",
      "Image for d / train : (607560, 2048), val : (150332, 2048)\n",
      "b cate for s / (5055885, 57) (1212813, 57)\n",
      "m cat for d / (5055885, 552) (1212813, 552)\n",
      "s cate for s / (5055885, 3191) (1212813, 3191)\n",
      "d cat for d / (607560, 405) (150332, 405)\n"
     ]
    }
   ],
   "source": [
    "X_product_s = h5py.File('./Taejin/X_product_for_s.h5py', 'r')\n",
    "X_maker_s = h5py.File('./Taejin/X_maker_for_s.h5py', 'r')\n",
    "X_brand_s = h5py.File('./Taejin/X_brand_for_s.h5py', 'r')\n",
    "X_model_s = h5py.File('./Taejin/X_model_for_s.h5py', 'r')\n",
    "X_image_s = h5py.File('./Taejin/X_image_for_s.h5py', 'r')\n",
    "\n",
    "X_product_d = h5py.File('./Taejin/X_product_for_d.h5py', 'r')\n",
    "X_maker_d = h5py.File('./Taejin/X_maker_for_d.h5py', 'r')\n",
    "X_brand_d = h5py.File('./Taejin/X_brand_for_d.h5py', 'r')\n",
    "X_model_d = h5py.File('./Taejin/X_model_for_d.h5py', 'r')\n",
    "X_image_d = h5py.File('./Taejin/X_image_for_d.h5py', 'r')\n",
    "\n",
    "y_b_cate_s = h5py.File('./Taejin/y_bcateid_rm_for_s.h5py', 'r')\n",
    "y_m_cate_s = h5py.File('./Taejin/y_mcateid_rm_for_s.h5py', 'r')\n",
    "y_s_cate_s = h5py.File('./Taejin/y_scateid_rm.h5py', 'r')\n",
    "\n",
    "y_b_cate_d = h5py.File('./Taejin/y_bcateid_rm_for_d.h5py', 'r')\n",
    "y_m_cate_d = h5py.File('./Taejin/y_mcateid_rm_for_d.h5py', 'r')\n",
    "y_s_cate_d = h5py.File('./Taejin/y_scateid_rm_for_d.h5py', 'r')\n",
    "y_d_cate_d = h5py.File('./Taejin/y_dcateid_rm.h5py', 'r')\n",
    "\n",
    "print(\"Product for s / train : {}, val : {}\".format(X_product_s['train'].shape, X_product_s['val'].shape))\n",
    "print(\"Maker for s / train : {}, val : {}\".format(X_maker_s['train'].shape, X_maker_s['val'].shape))\n",
    "print(\"Brand for s / train : {}, val : {}\".format(X_brand_s['train'].shape, X_brand_s['val'].shape))\n",
    "print(\"Model for s / train : {}, val : {}\".format(X_model_s['train'].shape, X_model_s['val'].shape))\n",
    "print(\"Image for s / train : {}, val : {}\".format(X_image_s['train'].shape, X_image_s['val'].shape))\n",
    "\n",
    "print(\"Product for d / train : {}, val : {}\".format(X_product_d['train'].shape, X_product_d['val'].shape))\n",
    "print(\"Maker for d / train : {}, val : {}\".format(X_maker_d['train'].shape, X_maker_d['val'].shape))\n",
    "print(\"Brand for d / train : {}, val : {}\".format(X_brand_d['train'].shape, X_brand_d['val'].shape))\n",
    "print(\"Model for d / train : {}, val : {}\".format(X_model_d['train'].shape, X_model_d['val'].shape))\n",
    "print(\"Image for d / train : {}, val : {}\".format(X_image_d['train'].shape, X_image_d['val'].shape))\n",
    "\n",
    "print(\"b cate for s / {} {}\".format(y_b_cate_s['train'].shape, y_b_cate_s['val'].shape))\n",
    "print(\"m cat for d / {} {}\".format(y_m_cate_s['train'].shape, y_m_cate_s['val'].shape))\n",
    "print(\"s cate for s / {} {}\".format(y_s_cate_s['train'].shape, y_s_cate_s['val'].shape))\n",
    "print(\"d cat for d / {} {}\".format(y_d_cate_d['train'].shape, y_d_cate_d['val'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "max_len = 32\n",
    "embed_size = 128\n",
    "cate_size = 3191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:      165060836     7608712    79500524         868    77951600   155939796\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 32, 128)      12800000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 128)      12800000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 32, 128)      12800000    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 32, 128)      12800000    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 4096)         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 4096)         0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 4096)         0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 4096)         0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1048832     reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          1048832     reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1280)         0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1280)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          655872      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          131328      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 3191)         820087      dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 57,527,159\n",
      "Trainable params: 57,527,159\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = generate_model(max_features, max_len, embed_size, cate_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.35.75.2:8470') for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8977423443693438216)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 15150836544296562072)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1801436362513015490)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 6890304418270238500)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10778465270698302976)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 16817672864726212975)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7048409046845592281)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6801566380812397609)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6725649897635845110)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5042702329889293897)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11425570852743245574)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 12190445503905905897)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpu_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tpu_model.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = data_generator([X_product['train'], X_model['train'], X_brand['train'], X_maker['train'], X_image['train']], y_m_cate['train'], batch_size, len(X_product['train']))\n",
    "# valid_generator = data_generator([X_product['val'], X_model['val'], X_brand['val'], X_maker['val'],   X_image['val']], y_m_cate['val'], batch_size, len(X_product['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = data_generator([X_product_d['train'], X_model_d['train'], X_brand_d['train'], X_maker_d['train'], X_image_d['train']], y_d_cate_d['train'], batch_size, len(X_product_d['train']))\n",
    "# valid_generator = data_generator([X_product_d['val'], X_model_d['val'], X_brand_d['val'], X_maker_d['val'], X_image_d['val']], y_d_cate_d['val'], batch_size, len(X_product_d['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator([X_product_s['train'], X_model_s['train'], X_brand_s['train'], X_maker_s['train'], X_image_s['train']], y_s_cate_s['train'], batch_size, len(X_product_s['train']))\n",
    "valid_generator = data_generator([X_product_s['val'], X_model_s['val'], X_brand_s['val'], X_maker_s['val'], X_image_s['val']], y_s_cate_s['val'], batch_size, len(X_product_s['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('./Taejin/s_model_rm.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tpu_model.fit_generator(generator=train_generator, \n",
    "                        steps_per_epoch=int(len(X_product_s['train'])/batch_size)+1, \n",
    "                        validation_data=valid_generator, \n",
    "                        validation_steps=int(len(X_product_s['val'])/batch_size)+1, epochs=20,\n",
    "                        callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "load_b = load_model('b_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 32, 128)      25600000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 128)      25600000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 32, 128)      25600000    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 32, 128)      25600000    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 4096)         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 4096)         0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 4096)         0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 4096)         0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          524544      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1048832     reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          1048832     reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1280)         0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          655872      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          131328      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 57)           14649       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,922,745\n",
      "Trainable params: 107,922,233\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "load_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpu_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "load_b.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5369962b74e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_product = np.array(X_job_product['train'])\n",
    "np_maker = np.array(X_job_maker['train'])\n",
    "np_brand = np.array(X_job_brand['train'])\n",
    "np_model = np.array(X_job_model['train'])\n",
    "np_image = np.array(X_job_image['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_y_job = np.array(y_b_job['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(0.2) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7278c1161fa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjob_product_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_product_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_maker_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_maker_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_brand_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_brand_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_model_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_model_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_image_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_image_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b_job_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b_job_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_product\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_maker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_brand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_y_job\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 142\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(0.2) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "job_product_train, job_product_val, job_maker_train, job_maker_val, job_brand_train, job_brand_val, job_model_train, job_model_val, job_image_train, job_image_val, y_b_job_train, y_b_job_val = train_test_split(np_product, np_maker, np_brand, np_model, np_image, np_y_job, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator([X_job_product['train'], X_job_model['train'], X_job_brand['train'], X_job_maker['train'], X_job_image['train']], y_b_job['train'], batch_size, len(X_job_product['train']))\n",
    "valid_generator = data_generator([X_product['val'], X_model['val'], X_brand['val'], X_maker['val'], X_image['val']], y_b_cate['val'], batch_size, len(X_product['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('b_model_job.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_b.fit_generator(generator=train_generator, \n",
    "                        steps_per_epoch=int(len(X_job_product['train'])/batch_size)+1, \n",
    "                        validation_data=valid_generator, \n",
    "                        validation_steps=int(len(X_product['val'])/batch_size)+1, epochs=20,\n",
    "                        callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in load_b.layers:\n",
    "    print(i.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_b.layers[-6].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_b = load_model('tpu_b_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_b.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_m_input = Input(shape=(1,), name='m_input')\n",
    "new_m = Dense(256, activation='relu', name='dense_1_m1')(new_m_input)\n",
    "concate_m = Concatenate(name='concate_m1')([load_b.layers[-6].output, new_m])\n",
    "# new_m = BatchNormalization()(concate_m)\n",
    "new_m = Dense(512, activation='relu', name='dense_2_m1')(concate_m)\n",
    "new_m = Dropout(0.2, name='drop_2')(new_m)\n",
    "new_m = Dense(256, activation='relu', name='dense_4_m1')(new_m)\n",
    "new_m = Dropout(0.2, name='drop_3')(new_m)\n",
    "output = Dense(552, activation='softmax', name='dense_3_m1')(new_m)\n",
    "\n",
    "model_m = Model([load_b.layers[4].input,load_b.layers[5].input,load_b.layers[6].input,load_b.layers[7].input,load_b.layers[8].input,  new_m_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_m(list_X,input_cate, y, batch_size, total_size):\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    while True:\n",
    "        if (start_idx + batch_size) >= total_size:\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "            \n",
    "        end_idx += batch_size\n",
    "#         offset = np.random.randint(0, total_size - batch_size)\n",
    "        batch_X = []\n",
    "        for X in list_X:\n",
    "            batch_X.append(np.array(X[start_idx:end_idx]))\n",
    "            \n",
    "        cate_input = np.argmax(input_cate[start_idx:end_idx], axis=1)+1\n",
    "        batch_X.append(cate_input)    \n",
    "        batch_y = np.array(y[start_idx:end_idx])\n",
    "        start_idx = end_idx        \n",
    "        \n",
    "        yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_m = data_generator_m([X_product['train'], X_model['train'], X_brand['train'], X_maker['train'], X_image['train']],y_b_cate['train'], y_m_cate['train'], batch_size, len(X_product['train']))\n",
    "valid_generator_m = data_generator_m([X_product['val'], X_model['val'], X_brand['val'], X_maker['val'],   X_image['val']],y_b_cate['val'], y_m_cate['val'], batch_size, len(X_product['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_generator_m)[0][-1][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['acc'])\n",
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "ft_checkpoint = ModelCheckpoint('m_model_transfer.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.fit_generator(generator=train_generator_m, \n",
    "                        steps_per_epoch=int(6507854/batch_size)+1, \n",
    "                        validation_data=valid_generator_m, \n",
    "                        validation_steps=int(1626964/batch_size)+1, epochs=10,\n",
    "                        callbacks=[ft_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.save('tpu_m_transfer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S_transfer model test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "load_m = load_model('m_model_transfer.h5')\n",
    "load_m.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_2_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_3_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_4_1:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_5_1:0\", shape=(?, 2048), dtype=float32)\n",
      "Tensor(\"m_input:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concate_m1/concat:0\", shape=(?, 1536), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(load_m.layers[0].input)\n",
    "print(load_m.layers[1].input)\n",
    "print(load_m.layers[2].input)\n",
    "print(load_m.layers[3].input)\n",
    "print(load_m.layers[8].input)\n",
    "print(load_m.layers[-9].input)\n",
    "print(load_m.layers[-6].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_s = Input(shape=(1,), name='input_s')\n",
    "new1 = Dense(256, activation='relu')(new_s)\n",
    "concate_m = Concatenate()([load_m.layers[-6].output, new1])\n",
    "# new1 = BatchNormalization()(concate_m)\n",
    "new1 = Dense(256, activation='relu')(concate_m)\n",
    "new1 = Dropout(0.2)(new1)\n",
    "output = Dense(3191, activation='softmax')(new1)\n",
    "\n",
    "model_s = Model([load_m.layers[0].input,load_m.layers[1].input,load_m.layers[2].input,load_m.layers[3].input,load_m.layers[8].input, load_m.layers[-9].input, new_s], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "max_len = 32\n",
    "embed_size = 128\n",
    "cate_size = 3191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_s(list_X,input_cate_b, input_cate_m, y, batch_size, total_size):\n",
    "#     start_idx = 0\n",
    "#     end_idx = 0\n",
    "    while True:\n",
    "#         end_idx += batch_size\n",
    "        offset = np.random.randint(0, total_size - batch_size)\n",
    "        batch_X = []\n",
    "        for X in list_X:\n",
    "            batch_X.append(np.array(X[offset:offset+batch_size]))\n",
    "            \n",
    "        cate_input_b = np.argmax(input_cate_b[offset:offset+batch_size], axis=1)+1\n",
    "        batch_X.append(cate_input_b)\n",
    "        cate_input_m = np.argmax(input_cate_m[offset:offset+batch_size], axis=1)+1\n",
    "        batch_X.append(cate_input_m)\n",
    "        batch_y = np.array(y[offset:offset+batch_size])\n",
    "#         start_idx = end_idx        \n",
    "        \n",
    "        yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b_cate_s = h5py.File('./Taejin/y_bcateid_rm_for_s.h5py', 'r')\n",
    "y_m_cate_s = h5py.File('./Taejin/y_mcateid_rm_for_s.h5py', 'r')\n",
    "y_s_cate_s = h5py.File('./Taejin/y_scateid_rm.h5py', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_s = data_generator_s([X_product_s['train'], X_model_s['train'], X_brand_s['train'], X_maker_s['train'], X_image_s['train']],y_b_cate_s['train'], y_m_cate_s['train'],y_s_cate_s['train'], batch_size, len(X_product_s['train']))\n",
    "valid_generator_s = data_generator_s([X_product_s['val'], X_model_s['val'], X_brand_s['val'], X_maker_s['val'], X_image_s['val']],y_b_cate_s['val'], y_m_cate_s['val'], y_s_cate_s['val'], batch_size, len(X_product_s['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['accuracy'])\n",
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "s_checkpoint = ModelCheckpoint('s_model_rm.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 1.9674 - acc: 0.5543\n",
      "Epoch 00001: acc improved from -inf to 0.55557, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 801s 5s/step - loss: 1.9607 - acc: 0.5556 - val_loss: 0.7775 - val_acc: 0.7934\n",
      "Epoch 2/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 0.6921 - acc: 0.8079\n",
      "Epoch 00002: acc improved from 0.55557 to 0.80795, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 818s 5s/step - loss: 0.6916 - acc: 0.8079 - val_loss: 0.6123 - val_acc: 0.8349\n",
      "Epoch 3/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 0.5194 - acc: 0.8529\n",
      "Epoch 00003: acc improved from 0.80795 to 0.85295, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 861s 6s/step - loss: 0.5195 - acc: 0.8529 - val_loss: 0.5296 - val_acc: 0.8564\n",
      "Epoch 4/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 0.4371 - acc: 0.8751\n",
      "Epoch 00004: acc improved from 0.85295 to 0.87501, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 830s 5s/step - loss: 0.4375 - acc: 0.8750 - val_loss: 0.5107 - val_acc: 0.8618\n",
      "Epoch 5/10\n",
      "154/155 [============================>.] - ETA: 4s - loss: 0.3852 - acc: 0.8884\n",
      "Epoch 00005: acc improved from 0.87501 to 0.88851, saving model to s_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "155/155 [==============================] - 836s 5s/step - loss: 0.3850 - acc: 0.8885 - val_loss: 0.5124 - val_acc: 0.8639\n",
      "Epoch 6/10\n",
      " 26/155 [====>.........................] - ETA: 9:44 - loss: 0.3740 - acc: 0.8945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-ccd687d9f33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_product_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         callbacks=[s_checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_s.fit_generator(generator=train_generator_s, \n",
    "                        steps_per_epoch=int(len(X_product_s['train'])/batch_size)+1, \n",
    "                        validation_data=valid_generator_s, \n",
    "                        validation_steps=int(len(X_product_s['val'])/batch_size)+1, epochs=10,\n",
    "                        callbacks=[s_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D model test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "load_d = load_model('s_model_rm.h5')\n",
    "load_d.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in load_d.layers:\n",
    "    print(i.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_2_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_3_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_4_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"input_5_2:0\", shape=(?, 2048), dtype=float32)\n",
      "Tensor(\"m_input_1:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"input_s_3:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concatenate_2_2/concat:0\", shape=(?, 1792), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(load_d.layers[4].input)\n",
    "print(load_d.layers[5].input)\n",
    "print(load_d.layers[6].input)\n",
    "print(load_d.layers[7].input)\n",
    "print(load_d.layers[8].input)\n",
    "print(load_d.layers[-10].input)\n",
    "print(load_d.layers[-7].input)\n",
    "print(load_d.layers[-4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d = Input(shape=(1,), name='input_d')\n",
    "new1 = Dense(256, activation='relu')(new_d)\n",
    "concate_d = Concatenate()([load_d.layers[-4].output, new1])\n",
    "# new1 = BatchNormalization()(concate_m)\n",
    "new1 = Dense(256, activation='relu')(concate_d)\n",
    "new1 = Dropout(0.2)(new1)\n",
    "output = Dense(405, activation='softmax')(new1)\n",
    "\n",
    "model_d = Model([load_d.layers[4].input,load_d.layers[5].input,load_d.layers[6].input,load_d.layers[7].input,load_d.layers[8].input, load_d.layers[-10].input, load_d.layers[-7].input, new_d], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_d(list_X,input_cate_b, input_cate_m, input_cate_s, y, batch_size, total_size):\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    while True:\n",
    "        if (start_idx + batch_size) >= total_size:\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "        end_idx += batch_size\n",
    "#         offset = np.random.randint(0, total_size - batch_size)\n",
    "        batch_X = []\n",
    "        for X in list_X:\n",
    "            batch_X.append(np.array(X[start_idx:end_idx]))\n",
    "            \n",
    "        cate_input_b = np.argmax(input_cate_b[start_idx:end_idx], axis=1)+1\n",
    "        batch_X.append(cate_input_b)\n",
    "        cate_input_m = np.argmax(input_cate_m[start_idx:end_idx], axis=1)+1\n",
    "        batch_X.append(cate_input_m)\n",
    "        cate_input_s = np.argmax(input_cate_s[start_idx:end_idx], axis=1)\n",
    "        batch_X.append(cate_input_s)\n",
    "        batch_y = np.array(y[start_idx:end_idx])\n",
    "        start_idx = end_idx        \n",
    "        \n",
    "        yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_d = data_generator_d([X_product_d['train'], X_model_d['train'], X_brand_d['train'], X_maker_d['train'], X_image_d['train']],y_b_cate_d['train'], y_m_cate_d['train'],y_s_cate_d['train'], y_d_cate_d['train'], batch_size, len(X_product_d['train']))\n",
    "valid_generator_d = data_generator_d([X_product_d['val'], X_model_d['val'], X_brand_d['val'], X_maker_d['val'], X_image_d['val']],y_b_cate_d['val'], y_m_cate_d['val'], y_s_cate_d['val'], y_d_cate_d['val'], batch_size, len(X_product_d['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d.compile(loss='categorical_crossentropy', optimizer=tf.train.AdamOptimizer(), metrics=['accuracy'])\n",
    "# early_stopping = EarlyStopping(monitor='acc', mode = 'max',patience=5, verbose=1)\n",
    "\n",
    "d_checkpoint = ModelCheckpoint('d_model_rm.h5',monitor='acc', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='acc', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 2.6140 - acc: 0.5301\n",
      "Epoch 00001: acc improved from -inf to 0.53142, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 59s 395ms/step - loss: 2.6036 - acc: 0.5314 - val_loss: 1.5718 - val_acc: 0.6585\n",
      "Epoch 2/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 1.3651 - acc: 0.6809\n",
      "Epoch 00002: acc improved from 0.53142 to 0.68199, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 56s 374ms/step - loss: 1.3598 - acc: 0.6820 - val_loss: 1.0899 - val_acc: 0.7119\n",
      "Epoch 3/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 1.0747 - acc: 0.7331\n",
      "Epoch 00003: acc improved from 0.68199 to 0.73398, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 57s 379ms/step - loss: 1.0708 - acc: 0.7340 - val_loss: 0.9962 - val_acc: 0.7405\n",
      "Epoch 4/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.9064 - acc: 0.7631\n",
      "Epoch 00004: acc improved from 0.73398 to 0.76402, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 56s 378ms/step - loss: 0.9025 - acc: 0.7640 - val_loss: 0.9140 - val_acc: 0.7495\n",
      "Epoch 5/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.7593 - acc: 0.7926\n",
      "Epoch 00005: acc improved from 0.76402 to 0.79344, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 55s 367ms/step - loss: 0.7562 - acc: 0.7934 - val_loss: 0.7825 - val_acc: 0.7883\n",
      "Epoch 6/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.6503 - acc: 0.8174\n",
      "Epoch 00006: acc improved from 0.79344 to 0.81809, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 361ms/step - loss: 0.6476 - acc: 0.8181 - val_loss: 0.7688 - val_acc: 0.7901\n",
      "Epoch 7/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.8377\n",
      "Epoch 00007: acc improved from 0.81809 to 0.83837, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 364ms/step - loss: 0.5488 - acc: 0.8384 - val_loss: 0.6570 - val_acc: 0.8245\n",
      "Epoch 8/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8528\n",
      "Epoch 00008: acc improved from 0.83837 to 0.85343, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 365ms/step - loss: 0.4874 - acc: 0.8534 - val_loss: 0.6701 - val_acc: 0.8216\n",
      "Epoch 9/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8684\n",
      "Epoch 00009: acc improved from 0.85343 to 0.86903, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 365ms/step - loss: 0.4323 - acc: 0.8690 - val_loss: 0.6460 - val_acc: 0.8346\n",
      "Epoch 10/10\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8843\n",
      "Epoch 00010: acc improved from 0.86903 to 0.88483, saving model to d_model_rm.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "149/149 [==============================] - 54s 363ms/step - loss: 0.3760 - acc: 0.8848 - val_loss: 0.6423 - val_acc: 0.8353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feef1e174e0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d.fit_generator(generator=train_generator_d, \n",
    "                        steps_per_epoch=int(len(X_product_d['train'])/batch_size)+1, \n",
    "                        validation_data=valid_generator_d, \n",
    "                        validation_steps=int(len(X_product_d['val'])/batch_size)+1, epochs=10,\n",
    "                        callbacks=[d_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
